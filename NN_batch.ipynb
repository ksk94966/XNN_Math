{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_batch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVU6TT7W1q081Wrc8jnJXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksk94966/XNN_Math/blob/master/NN_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct1oP-ACslzA"
      },
      "source": [
        "import os.path\n",
        "import urllib.request\n",
        "import gzip\n",
        "import math\n",
        "import numpy  as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time                                                         #for calculating the total time for the program\n",
        "\n",
        "start_prgm_time = time.time()\n",
        "\n",
        "# data\n",
        "DATA_NUM_TRAIN         = 60000\n",
        "DATA_NUM_TEST          = 10000\n",
        "DATA_CHANNELS          = 1\n",
        "DATA_ROWS              = 28\n",
        "DATA_COLS              = 28\n",
        "DATA_CLASSES           = 10\n",
        "DATA_URL_TRAIN_DATA    = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
        "DATA_URL_TRAIN_LABELS  = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
        "DATA_URL_TEST_DATA     = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
        "DATA_URL_TEST_LABELS   = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
        "DATA_FILE_TRAIN_DATA   = 'train_data.gz'\n",
        "DATA_FILE_TRAIN_LABELS = 'train_labels.gz'\n",
        "DATA_FILE_TEST_DATA    = 'test_data.gz'\n",
        "DATA_FILE_TEST_LABELS  = 'test_labels.gz'\n",
        "\n",
        "# display\n",
        "DISPLAY_ROWS   = 8\n",
        "DISPLAY_COLS   = 4\n",
        "DISPLAY_COL_IN = 10\n",
        "DISPLAY_ROW_IN = 25\n",
        "DISPLAY_NUM    = DISPLAY_ROWS*DISPLAY_COLS\n",
        "\n",
        "# download\n",
        "if (os.path.exists(DATA_FILE_TRAIN_DATA)   == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_DATA,   DATA_FILE_TRAIN_DATA)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_LABELS) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_LABELS, DATA_FILE_TRAIN_LABELS)\n",
        "if (os.path.exists(DATA_FILE_TEST_DATA)    == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TEST_DATA,    DATA_FILE_TEST_DATA)\n",
        "if (os.path.exists(DATA_FILE_TEST_LABELS)  == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TEST_LABELS,  DATA_FILE_TEST_LABELS)\n",
        "\n",
        "# training data\n",
        "# unzip the file, skip the header, read the rest into a buffer and format to NCHW\n",
        "file_train_data   = gzip.open(DATA_FILE_TRAIN_DATA, 'r')\n",
        "file_train_data.read(16)\n",
        "buffer_train_data = file_train_data.read(DATA_NUM_TRAIN*DATA_ROWS*DATA_COLS)\n",
        "train_data        = np.frombuffer(buffer_train_data, dtype=np.uint8).astype(np.float32)\n",
        "train_data        = train_data.reshape(DATA_NUM_TRAIN, 1, DATA_ROWS, DATA_COLS)\n",
        "\n",
        "# training labels\n",
        "# unzip the file, skip the header, read the rest into a buffer and format to a vector\n",
        "file_train_labels   = gzip.open(DATA_FILE_TRAIN_LABELS, 'r')\n",
        "file_train_labels.read(8)\n",
        "buffer_train_labels = file_train_labels.read(DATA_NUM_TRAIN)\n",
        "train_labels        = np.frombuffer(buffer_train_labels, dtype=np.uint8).astype(np.int32)\n",
        "\n",
        "# testing data\n",
        "# unzip the file, skip the header, read the rest into a buffer and format to NCHW\n",
        "file_test_data   = gzip.open(DATA_FILE_TEST_DATA, 'r')\n",
        "file_test_data.read(16)\n",
        "buffer_test_data = file_test_data.read(DATA_NUM_TEST*DATA_ROWS*DATA_COLS)\n",
        "test_data        = np.frombuffer(buffer_test_data, dtype=np.uint8).astype(np.float32)\n",
        "test_data        = test_data.reshape(DATA_NUM_TEST, 1, DATA_ROWS, DATA_COLS)\n",
        "\n",
        "# testing labels\n",
        "# unzip the file, skip the header, read the rest into a buffer and format to a vector\n",
        "file_test_labels   = gzip.open(DATA_FILE_TEST_LABELS, 'r')\n",
        "file_test_labels.read(8)\n",
        "buffer_test_labels = file_test_labels.read(DATA_NUM_TEST)\n",
        "test_labels        = np.frombuffer(buffer_test_labels, dtype=np.uint8).astype(np.int32)\n",
        "\n",
        "#Printing dimensions\n",
        "# print(train_data.shape)   # (60000, 1, 28, 28)\n",
        "# print(train_labels.shape) # (60000,)\n",
        "# print(test_data.shape)    # (10000, 1, 28, 28)\n",
        "# print(test_labels.shape)  # (10000,)\n",
        "\n",
        "#hidden Layer1\n",
        "wh1 = np.random.rand(784,1000)/np.sqrt(784)\n",
        "bh1 = np.random.rand(1,1000)\n",
        "\n",
        "#hidden Layer2\n",
        "wh2 = np.random.rand(1000,100)/np.sqrt(1000)\n",
        "bh2 = np.random.rand(1,100)\n",
        "\n",
        "#Output Layer\n",
        "wo = np.random.rand(100,10)/np.sqrt(100)\n",
        "bo = np.random.rand(1,10)\n",
        "\n",
        "#Helper functions\n",
        "def multi1(v):\n",
        "  return np.dot(v,wh1)\n",
        "\n",
        "def multi2(v):\n",
        "  return np.dot(v,wh2)\n",
        "\n",
        "def multi3(v):\n",
        "  return np.dot(v,wo)\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def reluDerivative(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x /np.sum(e_x)\n",
        "\n",
        "def getLabelMatrix(i):\n",
        "  lab = np.zeros((1,10))\n",
        "  lab[0][i] = 1\n",
        "  return lab\n",
        "\n",
        "\n",
        "\n",
        "epoch_test_accuracy = dict()                                                      #For storing epoch testing accuracies for the respective epochs\n",
        "accuracy_label_encountered = dict()                               #For storing the respective label count encountered\n",
        "accuracy_label_correct = dict()                                         #For storing the correctly predicted respective label count \n",
        "\n",
        "for i in range(10):                                                                  #to\n",
        "  accuracy_label_encountered[i] = 0       \n",
        "\n",
        "for i in range(10):\n",
        "  accuracy_label_correct[i] = 0\n",
        "\n",
        "lr = 0.001                    # Learning Rate\n",
        "\n",
        "test_data =  (test_data/255).astype('float32')\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------testing function---------------------------------------------------------------------------------#\n",
        "\n",
        "def testingNN(testdata,testlabels):\n",
        "  count = 0;\n",
        "  total = 0; \n",
        "#Forward Pass\n",
        "  total_testing_loss = 0\n",
        "  for sample in range(10000):\n",
        "    a0 = testdata[sample]                                                                    #division by 255\n",
        "    a0 = a0.reshape(1,784)                                                                 #Reshaping ---> Vectorization\n",
        "    #print(bh1)\n",
        "\n",
        "    #Passing the sample through first hidden layer\n",
        "    zh1 = np.add(multi1(a0) ,bh1)                                                                    #Creating Zh1 which is addtion of weights and bias\n",
        "    ah1 = ReLU(zh1)                                                                                            #Generated hiddenlayer1 Activations\n",
        "    # print(ah1)\n",
        "\n",
        "    #passing the h1 sample through second hiddden layer\n",
        "    zh2 = np.add(multi2(ah1), bh2)                                                                   #Creating Zh2 which is addtion of weights and bias\n",
        "    ah2 = ReLU(zh2)                                                                                             #Generated hiddenlayer2 Activations \n",
        "    \n",
        "\n",
        "    #passing the h2 sample through third hiddden layer\n",
        "    zo = np.add(multi3(ah2) ,bo)                                                                        #Creating Zo which is addtion of weights and bias\n",
        "    ao = softmax(zo)                                                                                             #Generated output Activations\n",
        "    #print(ao)\n",
        "\n",
        "    pred = np.argmax(ao)\n",
        "\n",
        "    #print(pred)\n",
        "\n",
        "    y = getLabelMatrix(testlabels[sample])                          #desired output label\n",
        "\n",
        "    accuracy_label_encountered[testlabels[sample]] += 1\n",
        "    #print(y)\n",
        "\n",
        "    if(y[0][pred]):\n",
        "      count += 1\n",
        "      accuracy_label_correct[pred] += 1 \n",
        "    \n",
        "    total += 1\n",
        "\n",
        "  #print((count/total)*100)\n",
        "\n",
        "    testing_loss = np.sum(-y * np.log(ao))\n",
        "    total_testing_loss += testing_loss\n",
        "\n",
        "  return (count/total)*100,total_testing_loss\n",
        "\n",
        "#-------------------------------------------------------------------------------------Training---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# accuracy_label_train_encountered = dict()                               #For storing the respective train label count encountered\n",
        "# accuracy_label_train_correct = dict()                                         #For storing the correctly predicted respective train label count \n",
        "\n",
        "# for i in range(10):                                                                  \n",
        "#   accuracy_label_train_encountered[i] = 0       \n",
        "\n",
        "# for i in range(10):\n",
        "#   accuracy_label_train_correct[i] = 0\n",
        "\n",
        "epoch_train_accuracy = dict()\n",
        "\n",
        "epoch_train_loss = dict()\n",
        "epoch_test_loss = dict()\n",
        "\n",
        "train_data =  (train_data/255).astype('float32')\n",
        "\n",
        "#Training epochs \n",
        "for epoch in range(5):\n",
        "  epoch_number = epoch + 1\n",
        "  start = time.time()\n",
        "  train_LossPerEpoch = 0\n",
        "  count = 0;\n",
        "  total = 0; \n",
        "  #Training Forward Pass\n",
        "  batch = 0\n",
        "  dcost_dw1 =0 \n",
        "  dcost_db1 = 0\n",
        "  dcost_dw2 =0\n",
        "  dcost_db2 = 0\n",
        "  dcost_dwf =0\n",
        "  dcost_dbf = 0\n",
        "  for sample in range(60000):\n",
        "\n",
        "    batch += 1\n",
        "\n",
        "    a0 = np.copy(train_data[sample])                                               \n",
        "    a0 = a0.reshape(1,784)                                                                                #Reshaping ---> Vectorization\n",
        "    #print(bh1)\n",
        "\n",
        "    #Passing the sample through first hidden layer\n",
        "    zh1 = np.add(multi1(a0) ,bh1)                                                                    #Creating Zh1 which is addtion of weights and bias\n",
        "    ah1 = ReLU(zh1)                                                                                            #Generated hiddenlayer1 Activations\n",
        "    # print(ah1)\n",
        "\n",
        "    #passing the h1 sample through second hiddden layer\n",
        "    zh2 = np.add(multi2(ah1), bh2)                                                                   #Creating Zh2 which is addtion of weights and bias\n",
        "    ah2 = ReLU(zh2)                                                                                             #Generated hiddenlayer2 Activations \n",
        "    \n",
        "    #passing the sample to get output\n",
        "    zo = np.add(multi3(ah2) ,bo)                                                                        #Creating Zo which is addtion of weights and bias\n",
        "    ao = softmax(zo)                                                                                             #Generated output Activations\n",
        "    #print(ao)\n",
        "\n",
        "    pred = np.argmax(ao)\n",
        "\n",
        "    y = getLabelMatrix(train_labels[sample])                                              #desired output label\n",
        "\n",
        "    # accuracy_label_train_encountered[testlabels[sample]] += 1\n",
        "    # #print(y)\n",
        "\n",
        "    if(y[0][pred]):\n",
        "      count += 1\n",
        "      # accuracy_label_train_correct[pred] += 1 \n",
        "    \n",
        "    total += 1\n",
        "\n",
        "    #Here we are using entropy function as loss function\n",
        "    #BackPropagation----------------------------------------------------------\n",
        "\n",
        "    #phase 1              ---> for the output layer\n",
        "    dcost_dzo = ao - y                                                                             #(1,10)\n",
        "    dzo_dwo = ah2.transpose()                                                            #(1,100)   --> we have to transpose this one\n",
        "    dcost_dwo = np.dot( dzo_dwo,dcost_dzo)                                 #derivate of cost w.r.t output weights      #(100,10)\n",
        "    dcost_dbo = dcost_dzo                                                                   #derivative of cost w.r.t bias                      #(1,10)\n",
        "\n",
        "    #phase 2              ---> for the hidden layer 2 \n",
        "    dzo_dah2 = wo\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.transpose())\n",
        "    dah2_dzh2 = reluDerivative(zh2)\n",
        "    dzh2_dwh2 = ah1\n",
        "    dcost_dwh2 = np.dot(dzh2_dwh2.T,dah2_dzh2*dcost_dah2)\n",
        "    dcost_dbh2 = dcost_dah2 * dah2_dzh2\n",
        "    #print(dcost_dwh2)\n",
        "\n",
        "    #phase 3   ---- > for hidden layer 1\n",
        "    dzh2_dah1 = wh2\n",
        "    dcost_dzh2 = dcost_dah2 * dah2_dzh2\n",
        "    dcost_dah1 = np.dot(dcost_dzh2,dzh2_dah1.transpose()) \n",
        "    dah1_dzh1 = reluDerivative(zh1)\n",
        "    dzh1_dwh1 = a0\n",
        "    dcost_dwh1 = np.dot(dzh1_dwh1.T,dah1_dzh1 *dcost_dah1)\n",
        "    dcost_dbh1 = dcost_dah1 * dah1_dzh1\n",
        "    #print(dzh1_dwh1)\n",
        "    \n",
        "    batch_size = 20\n",
        "    #batch wise processing\n",
        "    if (batch!=0 and batch%batch_size==0):\n",
        "      wh1 -= lr * (dcost_dw1/batch_size)\n",
        "      bh1 -= lr * ((dcost_db1.sum(axis=0))/batch_size)\n",
        "      wh2 -= lr * (dcost_dw2/batch_size)\n",
        "      bh2 -= lr * ((dcost_db2.sum(axis=0))/batch_size)\n",
        "      wo -= lr * (dcost_dwf/batch_size)\n",
        "      bo -= lr * ((dcost_dbf.sum(axis=0))/batch_size)\n",
        "      dcost_dw1 =0 \n",
        "      dcost_db1 = 0\n",
        "      dcost_dw2 =0\n",
        "      dcost_db2 = 0\n",
        "      dcost_dwf =0\n",
        "      dcost_dbf = 0\n",
        "    else:\n",
        "      dcost_dw1 += dcost_dwh1\n",
        "      dcost_db1 += dcost_dbh1\n",
        "      dcost_dw2 += dcost_dwh2\n",
        "      dcost_db2 += dcost_dbh2\n",
        "      dcost_dwf += dcost_dwo\n",
        "      dcost_dbf += dcost_dbo\n",
        "\n",
        "    \n",
        "    loss = np.sum(-y * np.log(ao))\n",
        "    train_LossPerEpoch += loss\n",
        "    #print('Loss function value: ', loss)\n",
        "  training_accuracy_perEachEpoch = (count/total)*100\n",
        "  epoch_train_accuracy[epoch_number] = training_accuracy_perEachEpoch\n",
        "  end = time.time()\n",
        "  time_elapsed = end - start\n",
        "  print(\"Epoch Number \", epoch_number)\n",
        "  print(\"-------------------------------start--------------------------------\")\n",
        "  print(\"Time Elapsed --->     \", time_elapsed )\n",
        "  epoch_train_loss[epoch_number] = train_LossPerEpoch\n",
        "  Avg_train_LossPerEpoch = train_LossPerEpoch/10000\n",
        "  print(\"Average Training Loss for this epoch -->   \" , Avg_train_LossPerEpoch)\n",
        "  test_accuracy_epoch,test_LossPerEpoch =  testingNN(test_data,test_labels)\n",
        "  print(\"Average Testing Loss for this epoch -->   \" , test_LossPerEpoch/10000)\n",
        "  epoch_test_loss[epoch_number] = test_LossPerEpoch\n",
        "  print(\"Testing Accuracy for this epoch--->   \", test_accuracy_epoch)\n",
        "  print(\"---------------------End---------------------------------------\")\n",
        "  print(\"\\n\")\n",
        "  epoch_test_accuracy[epoch_number] = test_accuracy_epoch                                    #Storing the testing accuracy for each epoch to print the plot later at the end\n",
        "  \n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------Overall  Accuracy Display for each label------------------------------------------------- \n",
        "\n",
        "accuracy_display = dict()                                                                           #For storing epoch label accuracies for each respective label\n",
        "for i in range(10):\n",
        "    if(accuracy_label_encountered[i]!=0):\n",
        "      accuracy_display[i]  = accuracy_label_correct[i]/accuracy_label_encountered[i]\n",
        "    else:\n",
        "      accuracy_display[i] = 0  \n",
        "\n",
        "print(\"-----------------Displaying each labels Accuracies-----------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "for k in accuracy_display:\n",
        "  print(\"Label  \",k,\"  Accuracy -->  \", accuracy_display[k] )\n",
        "\n",
        "#print(accuracy_display)\n",
        "#final Value\n",
        "#plotting\n",
        "\n",
        "epoch_train_count = []                                                                          #Helper array\n",
        "epoch_trainAccuracy = []                                                             #Helper array\n",
        "for key in epoch_train_accuracy:                                                        #Storing all the accuracy values in the arrays to display a plot  \n",
        "  epoch_train_count.append(key)\n",
        "  epoch_trainAccuracy.append(epoch_train_accuracy[key])\n",
        "\n",
        "epoch_count = []                                                                          #Helper array\n",
        "epoch_testAccuracy = []                                                             #Helper array\n",
        "for key in epoch_test_accuracy:                                                        #Storing all the accuracy values in the arrays to display a plot  \n",
        "  epoch_count.append(key)\n",
        "  epoch_testAccuracy.append(epoch_test_accuracy[key])\n",
        "\n",
        "epoch_trainLoss = []\n",
        "for key in epoch_train_loss:\n",
        "  epoch_trainLoss.append(epoch_train_loss[key])\n",
        "\n",
        "epoch_testLoss = []\n",
        "for key in epoch_test_loss:\n",
        "  epoch_testLoss.append(epoch_test_loss[key])\n",
        "\n",
        "\n",
        "#Displaying the plot\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy vs Epoch\")\n",
        "print(\"\\n\")\n",
        "plt.plot(epoch_count, epoch_testAccuracy, 'r--',marker='o',label='Test Accuracy')\n",
        "\n",
        "plt.legend(['Test Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()      \n",
        "\n",
        "#performance Display plot\n",
        "#Displaying the Accuracy vs epoch for both training and testing data\n",
        "print(\"\\n\")\n",
        "print(\"Testing Accuracy, TrainingAccuracy vs Epoch\")\n",
        "print(\"\\n\")\n",
        "plt.plot(epoch_count, epoch_testAccuracy,'r--',marker='o',label='Test Accuracy')\n",
        "plt.plot(epoch_count, epoch_trainAccuracy, 'b--',marker='v',label='Train Accuracy')\n",
        "plt.legend(['Test Accuracy','Train Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()   \n",
        "\n",
        "#Displaying the Loss vs epoch for both training and testing data\n",
        "print(\"\\n\")\n",
        "print(\"Testing Accuracy, TrainingAccuracy vs Epoch\")\n",
        "print(\"\\n\")\n",
        "plt.plot(epoch_count, epoch_testLoss,'r--',marker='o',label='Test Accuracy')\n",
        "plt.plot(epoch_count, epoch_trainLoss, 'b--',marker='v',label='Train Accuracy')\n",
        "plt.legend(['Test Accuracy','Train Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()   \n",
        "\n",
        "#Displaying the total time taken for the program\n",
        "\n",
        "end_prgm_time = time.time()\n",
        "total_prgm_time = end_prgm_time - start_prgm_time\n",
        "print(\"Total time Taken for the entire Program : --->  \" ,total_prgm_time )\n",
        "\n",
        "\n",
        "print(\"Division                          -->        1 * 28 * 28      -->        1 * 28 * 28       -->       None               -->      28*28 \\n\")\n",
        "print(\"Vectorization                -->        1 * 28 * 28      -->        1 * 784              -->       None              -->      None \\n\")\n",
        "print(\"Matrix multiplication  -->        1 * 784             -->        1 * 1000            -->       784 * 1000    -->      784*1000 \\n\")\n",
        "print(\"Addition                        -->        1 * 1000           -->        1 * 1000            -->       1 * 1000        -->      1000 \\n\")\n",
        "print(\"ReLU                              -->        1 * 1000           -->        1 * 1000           -->       None               -->      1000 \\n\")\n",
        "print(\"Matrix multiplication -->        1 * 1000           -->        1 * 100              -->       1000 * 1000   -->      1000*100 \\n\")\n",
        "print(\"Addition                       -->        1 * 100             -->        1 * 100              -->       1 * 100             -->      100 \\n\")\n",
        "print(\"ReLU                             -->        1 * 100             -->        1 * 100              -->       None                 -->      100 \\n\")\n",
        "print(\"Matrix multiplication -->        1 * 100            -->        1 * 10                -->       100 * 10            -->      100*10 \\n\")\n",
        "print(\"Addition                       -->        1 * 10              -->        1 * 10                -->       1 * 10                  -->      10 \\n\")\n",
        "print(\"Softmax                        -->        1 * 10              -->        1 * 10               -->       None                   -->      10*10 \\n\")\n",
        "\n",
        "\n",
        "#For displaying the correct label from the given output\n",
        "def NN_label(testdata):\n",
        "  \n",
        "  sample = testlabel\n",
        "  #Forward Pass\n",
        "  a0 = testdata                                                                                   #division by 255\n",
        "  a0 = a0.reshape(1,784)                                                                 #Reshaping ---> Vectorization\n",
        "  #print(bh1)\n",
        "\n",
        "  #Passing the sample through first hidden layer\n",
        "  zh1 = np.add(multi1(a0) ,bh1)                                                                    #Creating Zh1 which is addtion of weights and bias\n",
        "  ah1 = ReLU(zh1)                                                                                            #Generated hiddenlayer1 Activations\n",
        "  # print(ah1)\n",
        "\n",
        "  #passing the h1 sample through second hiddden layer\n",
        "  zh2 = np.add(multi2(ah1), bh2)                                                                   #Creating Zh2 which is addtion of weights and bias\n",
        "  ah2 = ReLU(zh2)                                                                                             #Generated hiddenlayer2 Activations \n",
        "  \n",
        "  #passing the h2 sample through third hiddden layer\n",
        "  zo = np.add(multi3(ah2) ,bo)                                                                        #Creating Zo which is addtion of weights and bias\n",
        "  ao = softmax(zo)                                                                                             #Generated output Activations\n",
        "  #print(ao)\n",
        "\n",
        "  pred = np.argmax(ao)\n",
        "\n",
        "  return pred\n",
        "\n",
        "fig = plt.figure(figsize=(DISPLAY_COL_IN, DISPLAY_ROW_IN))\n",
        "ax  = []\n",
        "for i in range(DISPLAY_NUM):\n",
        "    img = test_data[i, :, :, :].reshape((DATA_ROWS, DATA_COLS))\n",
        "    ax.append(fig.add_subplot(DISPLAY_ROWS, DISPLAY_COLS, i + 1))\n",
        "    predicted = NN_label(test_data[i])\n",
        "    ax[-1].set_title('True: ' + str(test_labels[i]) + ' NN: ' + str(predicted))\n",
        "    plt.imshow(img, cmap='Greys')\n",
        "plt.show()\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO9IQVKde5Le"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}